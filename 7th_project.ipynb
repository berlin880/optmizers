{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "import libariries\n"
      ],
      "metadata": {
        "id": "Dz70FIVwNEYy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEudo1QUNAPK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression class  implementation\n"
      ],
      "metadata": {
        "id": "FRxaQGyZNSzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000, optimizer='gd',batch_size=32, regularization=0.1, early_stopping=10,lr_decay=0.0, random_state=None, verbose=True):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.optimizer = optimizer.lower()\n",
        "        self.batch_size = batch_size\n",
        "        self.regularization = regularization\n",
        "        self.early_stopping = early_stopping\n",
        "        self.lr_decay = lr_decay\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.losses = []\n",
        "        self.best_loss = np.inf\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        # Numerically stable sigmoid with clipping\n",
        "        return np.clip(1 / (1 + np.exp(-z)), 1e-15, 1 - 1e-15)\n",
        "\n",
        "    def initialize_parameters(self, n_features):\n",
        "        # Small random initialization\n",
        "        np.random.seed(self.random_state)\n",
        "        self.W = np.random.randn(n_features, 1) * 0.01\n",
        "        self.b = 0\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        m = y_true.shape[0]\n",
        "        log_loss = -np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
        "        l2_penalty = (self.regularization/(2*m)) * np.sum(self.W**2)\n",
        "        return log_loss + l2_penalty\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        z = X.dot(self.W) + self.b\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return (self.predict_proba(X) >= threshold).astype(int)\n",
        "\n",
        "    def update_parameters(self, X, y, current_lr):\n",
        "        m = X.shape[0]\n",
        "        y_pred = self.predict_proba(X)\n",
        "\n",
        "        # Gradient with L2 regularization\n",
        "        dw = (X.T.dot(y_pred - y) / m) + (self.regularization/m)*self.W\n",
        "        db = np.mean(y_pred - y)\n",
        "\n",
        "        self.W -= current_lr * dw\n",
        "        self.b -= current_lr * db\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y).reshape(-1,1)\n",
        "        self.initialize_parameters(X.shape[1])\n",
        "\n",
        "        no_improvement = 0\n",
        "        np.random.seed(self.random_state)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Learning rate decay\n",
        "            current_lr = self.learning_rate / (1 + self.lr_decay*epoch)\n",
        "\n",
        "            # Shuffle data for SGD and mini-batch\n",
        "            if self.optimizer in ['sgd', 'mini-batch']:\n",
        "                shuffle_idx = np.random.permutation(len(y))\n",
        "                X_shuffled, y_shuffled = X[shuffle_idx], y[shuffle_idx]\n",
        "            else:\n",
        "                X_shuffled, y_shuffled = X, y\n",
        "\n",
        "            # Update parameters\n",
        "            if self.optimizer == 'gd':\n",
        "                self.update_parameters(X_shuffled, y_shuffled, current_lr)\n",
        "            elif self.optimizer == 'sgd':\n",
        "                for i in range(X_shuffled.shape[0]):\n",
        "                    self.update_parameters(X_shuffled[i:i+1], y_shuffled[i:i+1], current_lr)\n",
        "            elif self.optimizer == 'mini-batch':\n",
        "                for i in range(0, X_shuffled.shape[0], self.batch_size):\n",
        "                    end = i + self.batch_size\n",
        "                    self.update_parameters(X_shuffled[i:end], y_shuffled[i:end], current_lr)\n",
        "            else:\n",
        "                raise ValueError(\"Optimizer must be 'gd', 'sgd', or 'mini-batch'\")\n",
        "\n",
        "            # Calculate and store loss\n",
        "            y_pred = self.predict_proba(X)\n",
        "            loss = self.compute_loss(y, y_pred)\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            # Early stopping check\n",
        "            if loss < self.best_loss:\n",
        "                self.best_loss = loss\n",
        "                no_improvement = 0\n",
        "            else:\n",
        "                no_improvement += 1\n",
        "\n",
        "            if self.early_stopping and no_improvement >= self.early_stopping:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "            # Progress reporting\n",
        "            if self.verbose and (epoch % 100 == 0 or epoch == self.epochs-1):\n",
        "                print(f\"Epoch {epoch+1}: Loss={loss:.4f}, LR={current_lr:.6f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        cm = confusion_matrix(y, y_pred)\n",
        "        metrics = {\n",
        "            'Accuracy': accuracy_score(y, y_pred),\n",
        "            'Precision': precision_score(y, y_pred),\n",
        "            'Recall': recall_score(y, y_pred),\n",
        "            'F1': f1_score(y, y_pred)\n",
        "        }\n",
        "\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(cm)\n",
        "        for name, value in metrics.items():\n",
        "            print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_loss(self):\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(self.losses, label='Training Loss')\n",
        "        plt.title(\"Learning Curve\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        if self.stopped_epoch > 0:\n",
        "            plt.axvline(self.stopped_epoch, color='r', linestyle='--',\n",
        "                       label='Early Stopping')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "ldw-BeslNKf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main code\n"
      ],
      "metadata": {
        "id": "T8KRRl1KNq90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# ðŸš€ MAIN EXECUTION\n",
        "# -------------------------\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=2000, n_features=12, n_informative=8,\n",
        "                           n_classes=2, random_state=42)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "# Optimizer comparison\n",
        "results = {}\n",
        "config = {\n",
        "    'learning_rate': 0.01,\n",
        "    'epochs': 1000,\n",
        "    'regularization': 0.2,\n",
        "    'early_stopping': 20,\n",
        "    'lr_decay': 0.005,\n",
        "    'verbose': False\n",
        "}\n",
        "\n",
        "for optimizer in ['gd', 'sgd', 'mini-batch']:\n",
        "    print(f\"\\n=== Training with {optimizer.upper()} ===\")\n",
        "\n",
        "    model = LogisticRegression(\n",
        "        optimizer=optimizer,\n",
        "        batch_size=64,\n",
        "        random_state=42,\n",
        "        **config\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    model.plot_loss()\n",
        "\n",
        "    print(\"\\nTest Set Performance:\")\n",
        "    metrics = model.evaluate(X_test, y_test)\n",
        "    results[optimizer] = metrics"
      ],
      "metadata": {
        "id": "J9udWoW0NuPO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}